{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mL_i6grp1UV"
   },
   "source": [
    "# ***B.Create and use your own corpora (plaintext, categorical)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVNUIz7stwDF"
   },
   "outputs": [],
   "source": [
    "!pip install essential_generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21Vah9ptp1B3"
   },
   "outputs": [],
   "source": [
    "from essential_generators import DocumentGenerator\n",
    "gen = DocumentGenerator()\n",
    "# Python code to create a file\n",
    "for i in range(5):\n",
    "    doc = gen.paragraph()\n",
    "    file = open(f'doc{i}.txt','w')\n",
    "    file.write(doc)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THVrOb03vpaJ"
   },
   "outputs": [],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZnLCRI8s2nd"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "corpus_root = '/content/'\n",
    "filelist = PlaintextCorpusReader(corpus_root,fileids=['doc0.txt','doc1.txt','doc2.txt','doc3.txt','doc4.txt'],encoding='ISO-8859-1')\n",
    "print ('\\n File list: \\n')\n",
    "print (filelist.fileids())\n",
    "\n",
    "print (filelist.root)\n",
    "\n",
    "'''display other information about each text, by looping over all the values of fileid\n",
    "corresponding to the filelist file identifiers listed earlier and then computing statistics\n",
    "for each text.'''\n",
    "\n",
    "print ('\\n\\nStatistics for each text:\\n')\n",
    "print('AvgWordLen\\tAvgSentenceLen\\tno.ofTimesEachWordAppearsOnAvg\\tFileName')\n",
    "\n",
    "for fileid in filelist.fileids():\n",
    "    num_chars = len(filelist.raw(fileid))\n",
    "    num_words = len(filelist.words(fileid))\n",
    "    num_sents = len(filelist.sents(fileid))\n",
    "    num_vocab = len(set([w.lower() for w in filelist.words(fileid)]))\n",
    "    print (int(num_chars/num_words),'\\t\\t\\t', int(num_words/num_sents),'\\t\\t\\t',int(num_words/num_vocab),'\\t\\t', fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1126,
     "status": "ok",
     "timestamp": 1680333955172,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "j35eSxAytSBL",
    "outputId": "690efb98-c211-41d0-dcd6-87095602fdbf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvHbUCXjz4F0"
   },
   "source": [
    "# **c. Study Conditional frequency distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-45Jnk3yiwg"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gK3LCBsvzNJA"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('inaugural')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DII5idoYzkUL"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('udhr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jd6qg4Eht42I"
   },
   "outputs": [],
   "source": [
    "text = ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
    "pairs = [('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ...]\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "fd = nltk.ConditionalFreqDist((genre, word)\n",
    "          for genre in brown.categories()\n",
    "          for word in brown.words(categories=genre))\n",
    "genre_word = [(genre, word)\n",
    "               for genre in ['news', 'romance']\n",
    "               for word in brown.words(categories=genre)]\n",
    "print(len(genre_word))\n",
    "print(genre_word[:4])\n",
    "print(genre_word[-4:])\n",
    "cfd = nltk.ConditionalFreqDist(genre_word)\n",
    "print(cfd)\n",
    "print(cfd.conditions())\n",
    "print(cfd['news'])\n",
    "print(cfd['romance'])\n",
    "print(list(cfd['romance']))\n",
    "from nltk.corpus import inaugural\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "          (target, fileid[:4])\n",
    "            for fileid in inaugural.fileids()\n",
    "            for w in inaugural.words(fileid)\n",
    "            for target in ['america', 'citizen']\n",
    "            if w.lower().startswith(target))\n",
    "from nltk.corpus import udhr\n",
    "languages = ['Chickasaw', 'English', 'German_Deutsch', 'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "            (lang, len(word))\n",
    "             for lang in languages\n",
    "             for word in udhr.words(lang + '-Latin1'))\n",
    "cfd.tabulate(conditions=['English', 'German_Deutsch'], samples=range(10), cumulative=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyjdD6k5xzm3"
   },
   "source": [
    "# **d. Study of tagged corpora with methods like tagged_sents, tagged_words.**  \n",
    " **bold text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 581,
     "status": "ok",
     "timestamp": 1683572850582,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "4QS9-a9SxfCM",
    "outputId": "ebf83471-512f-4c3a-f33f-0e890716ee8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sentence tokenization\n",
      "===================\n",
      " ['Hello!', 'My name is Sneha kadam.', \"Today you'll be learning NLTK.\"]\n",
      "\n",
      "word tokenization\n",
      "===================\n",
      "\n",
      "['Hello', '!']\n",
      "['My', 'name', 'is', 'Sneha', 'kadam', '.']\n",
      "['Today', 'you', \"'ll\", 'be', 'learning', 'NLTK', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "\n",
    "para = \"Hello! My name is Sneha kadam. Today you'll be learning NLTK.\"\n",
    "sents = tokenize.sent_tokenize(para)\n",
    "print(\"\\nsentence tokenization\\n===================\\n\",sents)\n",
    "\n",
    "print(\"\\nword tokenization\\n===================\\n\")\n",
    "for index in range(len(sents)):\n",
    "     words = tokenize.word_tokenize(sents[index])\n",
    "     print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwrG7mAHyIsy"
   },
   "source": [
    "**e. Write a program to find the most frequent noun tags.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAI6L_Glxgtr"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "text = nltk.word_tokenize(\"Nick likes to play football. Nick does not like to play cricket.\")\n",
    "tagged = nltk.pos_tag(text)\n",
    "print(tagged)\n",
    "\n",
    "# checking if it is a noun or not\n",
    "addNounWords = []\n",
    "count=0\n",
    "for words in tagged:\n",
    "       val = tagged[count][1]\n",
    "       if(val == 'NN' or val == 'NNS' or val == 'NNPS' or val == 'NNP'):\n",
    "                  addNounWords.append(tagged[count][0])\n",
    "                  count+=1\n",
    "\n",
    "print (addNounWords)\n",
    "\n",
    "temp = defaultdict(int)\n",
    "# memoizing count\n",
    "for sub in addNounWords:\n",
    "    for wrd in sub.split():\n",
    "       temp[wrd] += 1\n",
    "\n",
    "\n",
    "# getting max frequency\n",
    "res = max(temp, key=temp.get)\n",
    "  # printing result\n",
    "print(\"Word with maximum frequency : \" + str(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1234,
     "status": "ok",
     "timestamp": 1683573131027,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "QojsY1VZudaf",
    "outputId": "8ff2c3d7-6a2c-48a0-9b45-66f995ecd4c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86_OsPYuyNRt"
   },
   "source": [
    "# **f. Map Words to Properties Using Python Dictio**naries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 583,
     "status": "ok",
     "timestamp": 1683573249834,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "nRSoIjCIyp36",
    "outputId": "0a00e6d7-a104-4344-b779-5d1f0076152b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'brand': 'Ford', 'model': 'Mustang', 'year': 1964}\n",
      "Ford\n",
      "3\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "#creating and printing a dictionay by mapping word with its properties\n",
    "thisdict = {   \"brand\": \"Ford\",   \"model\": \"Mustang\",   \"year\": 1964 }\n",
    "print(thisdict)\n",
    "print(thisdict[\"brand\"])\n",
    "print(len(thisdict))\n",
    "print(type(thisdict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOyiAMOIzCWZ"
   },
   "source": [
    "# ** g. Study \n",
    "\n",
    "***i) DefaultTagger,***\n",
    "\n",
    "***ii) Regular expression tagger,*** \n",
    "\n",
    "***iii) UnigramTagger ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*** i) DefaultTagger ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3927,
     "status": "ok",
     "timestamp": 1683573257442,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "WwbDEJeozJDE",
    "outputId": "a1729581-120b-4314-b9cf-b82abedbd2b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-79cf7b976edc>:6: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(exptagger.evaluate (testsentences))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13198749536374715\n",
      "[[('Hi', 'NN'), (',', 'NN')], [('How', 'NN'), ('are', 'NN'), ('you', 'NN'), ('?', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import DefaultTagger\n",
    "exptagger = DefaultTagger('NN')\n",
    "from nltk.corpus import treebank\n",
    "testsentences = treebank.tagged_sents() [1000:]\n",
    "print(exptagger.evaluate (testsentences))\n",
    "\n",
    "#Tagging a list of sentences\n",
    "import nltk\n",
    "from nltk.tag import DefaultTagger\n",
    "exptagger = DefaultTagger('NN')\n",
    "print(exptagger.tag_sents([['Hi', ','], ['How', 'are', 'you', '?']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1683573263740,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "GpjQj7LrztEJ",
    "outputId": "a5033c31-64d4-4b32-b854-521c741ed988"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxJu79qLzzwL"
   },
   "source": [
    "**ii) Regular expression tagger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1683573266097,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "k6cjWqdKz8YZ",
    "outputId": "d89b3fb7-3622-43fc-e87a-16e20e8f28b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Regexp Tagger: size=9>\n",
      "[('The', 'AT'), ('Fulton', 'NN'), ('County', 'NN'), ('Grand', 'NN'), ('Jury', 'NN'), ('said', 'NN'), ('Friday', 'NN'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'NN'), (\"Atlanta's\", 'NNS'), ('recent', 'NN'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', 'NN'), ('no', 'NN'), ('evidence', 'NN'), (\"''\", 'NN'), ('that', 'NN'), ('any', 'NN'), ('irregularities', 'NNS'), ('took', 'NN'), ('place', 'NN'), ('.', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.tag import RegexpTagger\n",
    "test_sent = brown.sents(categories='news')[0]\n",
    "regexp_tagger = RegexpTagger(     [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "                                   (r'(The|the|A|a|An|an)$', 'AT'),   # articles\n",
    "                                   (r'.*able$', 'JJ'),                # adjectives\n",
    "                                   (r'.*ness$', 'NN'),                # nouns formed from adjectives\n",
    "                                   (r'.*ly$', 'RB'),                  # adverbs\n",
    "                                   (r'.*s$', 'NNS'),               # plural nouns\n",
    "                                   (r'.*ing$', 'VBG'),                # gerunds\n",
    "                                   (r'.*ed$', 'VBD'),                 # past tense verbs\n",
    "                                   (r'.*', 'NN')                      # nouns (default)\n",
    "                                    ])\n",
    "print(regexp_tagger)\n",
    "print(regexp_tagger.tag(test_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmA-ntA_0sZr"
   },
   "source": [
    "***iii) UnigramTagger***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 365,
     "status": "ok",
     "timestamp": 1683573270355,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "i79zzrq-0uel",
    "outputId": "3dcd2f09-69fc-4c5c-cc18-d289a3eac184"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "\n",
      " [('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "\n",
      " [('Pierre', 'NN'), ('Vinken', None), (',', None), ('61', None), ('years', None), ('old', None), (',', None), ('will', None), ('join', None), ('the', None), ('board', None), ('as', None), ('a', None), ('nonexecutive', None), ('director', None), ('Nov.', None), ('29', None), ('.', None)]\n"
     ]
    }
   ],
   "source": [
    "# Loading Libraries\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "# Training using first 10 tagged sentences of the treebank corpus as data.\n",
    "# Using data\n",
    "train_sents = treebank.tagged_sents()[:10]\n",
    "# Initializing\n",
    "tagger = UnigramTagger(train_sents)\n",
    "# Lets see the first sentence\n",
    "# (of the treebank corpus) as list\n",
    "print(treebank.sents()[0])\n",
    "print('\\n',tagger.tag(treebank.sents()[0]))\n",
    "\n",
    "#Finding the tagged results after training.\n",
    "tagger.tag(treebank.sents()[0])\n",
    "\n",
    "#Overriding the context model\n",
    "tagger = UnigramTagger(model ={'Pierre': 'NN'})\n",
    "print('\\n',tagger.tag(treebank.sents()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRbS4yTp2Rg1"
   },
   "source": [
    "# **h. Find different words from a given plain text without any space by comparing this text with a given corpus of words. Also find the score of words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40883,
     "status": "ok",
     "timestamp": 1683574813459,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "ETfYFoME88g4",
    "outputId": "f5be0cd5-066a-412b-b2f2-44459748e59c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MENU\n",
      "------------------------------------------------------------\n",
      " 1 . Hash tag segmentation \n",
      " 2 . URL segmentation \n",
      " 9 . EXIT \n",
      "enter the input choice for performing word segmentation :- 1\n",
      "------------------------------------------------------------\n",
      "input with HashTag #whatismyname\n",
      "whatismyname\n",
      "what -appears in the corpus\n",
      "is -appears in the corpus\n",
      "my -appears in the corpus\n",
      "name -appears in the corpus\n",
      "------------------------------------------------------------\n",
      "output\n",
      "------------------------------------------------------------\n",
      "what is my name\n",
      "Score 4.0\n",
      "MENU\n",
      "------------------------------------------------------------\n",
      " 1 . Hash tag segmentation \n",
      " 2 . URL segmentation \n",
      " 9 . EXIT \n",
      "enter the input choice for performing word segmentation :- 2\n",
      "------------------------------------------------------------\n",
      "input with URL www.whatismyname.com\n",
      "whatismyname\n",
      "what -appears in the corpus\n",
      "is -appears in the corpus\n",
      "my -appears in the corpus\n",
      "name -appears in the corpus\n",
      "------------------------------------------------------------\n",
      "output\n",
      "------------------------------------------------------------\n",
      "what is my name\n",
      "Score 4.0\n",
      "MENU\n",
      "------------------------------------------------------------\n",
      " 1 . Hash tag segmentation \n",
      " 2 . URL segmentation \n",
      " 9 . EXIT \n",
      "enter the input choice for performing word segmentation :- 9\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from __future__ import with_statement #with statement for reading file\n",
    "import re # Regular expression\n",
    "\n",
    "while(True):\n",
    "    words = [] # corpus file words\n",
    "    testword = [] # test words\n",
    "    ans = [] # words matches with corpus\n",
    "\n",
    "    print(\"MENU\")\n",
    "    print(\"--\"*30)\n",
    "    print(\" 1 . Hash tag segmentation \")\n",
    "    print(\" 2 . URL segmentation \")\n",
    "    print(\" 9 . EXIT \")\n",
    "\n",
    "    choice = int(input(\"enter the input choice for performing word segmentation :- \"))\n",
    "    print(\"--\"*30)\n",
    "\n",
    "    if choice == 1:\n",
    "        text = \"#whatismyname\" # hash tag test data to segment\n",
    "        print(\"input with HashTag\",text)\n",
    "        pattern=re.compile(\"[^\\w']\")\n",
    "        a = pattern.sub('', text)\n",
    "    elif choice == 2:\n",
    "        text = \"www.whatismyname.com\" # url test data to segment\n",
    "        print(\"input with URL\",text)\n",
    "        a=re.split('\\s|(?<!\\d)[,.](?!\\d)', text)\n",
    "        splitwords = [\"www\",\"com\",\"in\"] # remove the words which is containg in the list\n",
    "        a =\"\".join([each for each in a if each not in splitwords])\n",
    "    elif choice == 9:\n",
    "        break\n",
    "    else:\n",
    "        print(\"wrong choice...try again\")\n",
    "    print(a)\n",
    "\n",
    "    for each in a:\n",
    "        testword.append(each) #test word\n",
    "    test_lenth = len(testword) # lenth of the test data\n",
    "\n",
    "    # Reading the corpus\n",
    "    with open('words.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        words =[(e.strip()) for e in lines]\n",
    "\n",
    "    def Seg(a,lenth):\n",
    "        ans =[]\n",
    "        for k in range(0,lenth+1): # this loop checks char by char in the corpus\n",
    "\n",
    "            if a[0:k] in words:\n",
    "                print(a[0:k],\"-appears in the corpus\")\n",
    "                ans.append(a[0:k])\n",
    "                break\n",
    "        if ans != []:\n",
    "            g = max(ans,key=len)\n",
    "            return g\n",
    "\n",
    "\n",
    "    test_tot_itr = 0 #each iteration value\n",
    "    answer = [] # Store the each word contains the corpus\n",
    "    Score = 0 # initial value for score\n",
    "    N = 37 # total no of corpus\n",
    "    M = 0\n",
    "    C = 0\n",
    "    while test_tot_itr < test_lenth:\n",
    "        ans_words = Seg(a,test_lenth)\n",
    "        if ans_words != 0:\n",
    "            test_itr = len(ans_words)\n",
    "            answer.append(ans_words)\n",
    "            a = a[test_itr:test_lenth]\n",
    "            test_tot_itr += test_itr\n",
    "    Aft_Seg = \" \".join([each for each in answer])\n",
    "    # print segmented words in the list\n",
    "    print(\"--\"*30)\n",
    "    print(\"output\")\n",
    "    print(\"--\"*30)\n",
    "    print(Aft_Seg) # print After segmentation the input\n",
    "    # Calculating Score\n",
    "    C = len(answer)\n",
    "    score = C * N / N # Calculate the score\n",
    "    print(\"Score\",score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPDUgMCb2MrOq8HQki/RO8o",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
