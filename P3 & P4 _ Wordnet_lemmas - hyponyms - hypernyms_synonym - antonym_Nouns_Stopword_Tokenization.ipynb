{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xi1r8VGyDyp6"
   },
   "source": [
    "# Practical - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rLVJZPV69Qf"
   },
   "source": [
    "*** a. Study of Wordnet Dictionary with methods as synsets, definitions, examples, antonyms ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6517,
     "status": "ok",
     "timestamp": 1681898884450,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "GVorUaQf65Sa",
    "outputId": "65e14dc6-3be2-49a4-936f-3ea3f086ac14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('computer.n.01'), Synset('calculator.n.01')]\n",
      "a machine for performing calculations automatically\n",
      "Examples: []\n",
      "[Lemma('sell.v.01.sell')]\n"
     ]
    }
   ],
   "source": [
    "'''WordNet provides synsets which is the collection of synonym words also called “lemmas”'''\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "print(wordnet.synsets(\"computer\"))\n",
    "\n",
    "# definition and example of the word ‘computer’\n",
    "print(wordnet.synset(\"computer.n.01\").definition())\n",
    "\n",
    "#examples\n",
    "print(\"Examples:\", wordnet.synset(\"computer.n.01\").examples())\n",
    "\n",
    "#get Antonyms\n",
    "print(wordnet.lemma('buy.v.01.buy').antonyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1681898901992,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "xUQxbqZj7Ytr",
    "outputId": "0d76f564-c327-42f2-8d6a-7c27d948d26a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSafm4pL7rVc"
   },
   "source": [
    "*** b. Study lemmas, hyponyms, hypernyms. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1681898904812,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "fJK5h3KB7zCH",
    "outputId": "ab707af7-b2b1-4e13-cd1e-b4e039ba0c51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('computer.n.01'), Synset('calculator.n.01')]\n",
      "['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system']\n",
      "Synset('computer.n.01') --> ['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system']\n",
      "Synset('calculator.n.01') --> ['calculator', 'reckoner', 'figurer', 'estimator', 'computer']\n",
      "[Lemma('computer.n.01.computer'), Lemma('computer.n.01.computing_machine'), Lemma('computer.n.01.computing_device'), Lemma('computer.n.01.data_processor'), Lemma('computer.n.01.electronic_computer'), Lemma('computer.n.01.information_processing_system')]\n",
      "Synset('computer.n.01')\n",
      "computing_device\n",
      "<bound method _WordNetObject.hyponyms of Synset('computer.n.01')>\n",
      "['analog_computer', 'analogue_computer', 'digital_computer', 'home_computer', 'node', 'client', 'guest', 'number_cruncher', 'pari-mutuel_machine', 'totalizer', 'totaliser', 'totalizator', 'totalisator', 'predictor', 'server', 'host', 'Turing_machine', 'web_site', 'website', 'internet_site', 'site']\n",
      "[Synset('vehicle.n.01')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "print(wordnet.synsets(\"computer\"))\n",
    "print(wordnet.synset(\"computer.n.01\").lemma_names()) #all lemmas for each synset.\n",
    "for e in wordnet.synsets(\"computer\"):\n",
    "   print(f'{e} --> {e.lemma_names()}')\n",
    "\n",
    "#print all lemmas for a given synset\n",
    "print(wordnet.synset('computer.n.01').lemmas())\n",
    "\n",
    "#get the synset corresponding to lemma\n",
    "print(wordnet.lemma('computer.n.01.computing_device').synset())\n",
    "\n",
    "#Get the name of the lemma\n",
    "print(wordnet.lemma('computer.n.01.computing_device').name())\n",
    "#Hyponyms give abstract concepts of the word that are much more specific\n",
    "#the list of hyponyms words of the computer\n",
    "syn = wordnet.synset('computer.n.01')\n",
    "print(syn.hyponyms)\n",
    "print([lemma.name() for synset in syn.hyponyms() for lemma in synset.lemmas()])\n",
    "\n",
    "#the semantic similarity in WordNet\n",
    "vehicle = wordnet.synset('vehicle.n.01')\n",
    "car = wordnet.synset('car.n.01')\n",
    "print(car.lowest_common_hypernyms(vehicle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QwGNkjj_MUm"
   },
   "source": [
    "***c.Write a program using python to find synonym and antonym of word \"active\" using Wordnet. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 489,
     "status": "ok",
     "timestamp": 1681898984234,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "PXxp5JoG_WDi",
    "outputId": "25df9327-b2ac-4920-f52c-e7aefbadde4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('active_agent.n.01'), Synset('active_voice.n.01'), Synset('active.n.03'), Synset('active.a.01'), Synset('active.s.02'), Synset('active.a.03'), Synset('active.s.04'), Synset('active.a.05'), Synset('active.a.06'), Synset('active.a.07'), Synset('active.s.08'), Synset('active.a.09'), Synset('active.a.10'), Synset('active.a.11'), Synset('active.a.12'), Synset('active.a.13'), Synset('active.a.14')]\n",
      "[Lemma('inactive.a.02.inactive')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "print( wordnet.synsets(\"active\"))\n",
    "\n",
    "print(wordnet.lemma('active.a.01.active').antonyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQnbxg9o_dxw"
   },
   "source": [
    "***d. Compare two nouns ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1681899082933,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "eVk6cnFW_nhU",
    "outputId": "eb72827b-f3f2-4964-c5f1-4c0f3291139f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path similarity of: \n",
      "Synset('football.n.01') ( n ) [ any of various games played with a ball (round or oval) in which two teams try to kick or carry or propel the ball into each other's goal ]\n",
      "Synset('soccer.n.01') ( n ) [ a football game in which two teams of 11 players try to kick or head a ball into the opponents' goal ]\n",
      "   is 0.5\n",
      "\n",
      "Path similarity of: \n",
      "Synset('football.n.02') ( n ) [ the inflated oblong ball used in playing American football ]\n",
      "Synset('soccer.n.01') ( n ) [ a football game in which two teams of 11 players try to kick or head a ball into the opponents' goal ]\n",
      "   is 0.05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "syn1 = wordnet.synsets('football')\n",
    "syn2 = wordnet.synsets('soccer')\n",
    "\n",
    "# A word may have multiple synsets, so need to compare each synset of word1 with synset of word2\n",
    "for s1 in syn1:\n",
    "  for s2 in syn2:\n",
    "    print(\"Path similarity of: \")\n",
    "    print(s1, '(', s1.pos(), ')', '[', s1.definition(), ']')\n",
    "    print(s2, '(', s2.pos(), ')', '[', s2.definition(), ']')\n",
    "    print(\"   is\", s1.path_similarity(s2))\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVE7IZnI_2Sf"
   },
   "source": [
    "***e. Handling stopword: \n",
    "\n",
    "***i) Using nltk Adding or Removing Stop Words in NLTK's Default Stop Word List ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1681899228533,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "kPhD_B0C_7lK",
    "outputId": "8821959d-811d-48ae-a2ed-406e0f2476a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yashesh', 'likes', 'play', 'football', ',', 'fond', 'tennis', '.']\n",
      "['Yashesh', 'likes', 'football', ',', 'however', 'fond', 'tennis', '.']\n",
      "['Yashesh', 'likes', 'football', ',', 'however', 'fond', 'tennis', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Yashesh likes to play football, however he is not too fond of tennis.\"\n",
    "text_tokens = word_tokenize(text)\n",
    "\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "\n",
    "print(tokens_without_sw)\n",
    "\n",
    "#add the word play to the NLTK stop word collection\n",
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.append('play')\n",
    "\n",
    "text_tokens = word_tokenize(text)\n",
    "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
    "\n",
    "print(tokens_without_sw)\n",
    "\n",
    "#remove ‘not’ from stop word collection all_stopwords.remove('not')\n",
    "\n",
    "text_tokens = word_tokenize(text)\n",
    "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
    "\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 576,
     "status": "ok",
     "timestamp": 1681899220764,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "0bo8xGjkAUh4",
    "outputId": "a80920b2-7c5c-4b8d-e226-9d635c401645"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdhzROnkAayq"
   },
   "source": [
    "***ii) Using Gensim Adding and Removing Stop Words in Default Gensim Stop Words List ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1221,
     "status": "ok",
     "timestamp": 1681899416164,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "2XXFsbRhAfyt",
    "outputId": "7f8c80ed-efe8-4700-acae-b141ac3925a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yashesh likes play football, fond tennis.\n",
      "frozenset({'our', 'last', 'no', 'anywhere', 'now', 'between', 'just', 'however', 'often', 'besides', 'per', 'me', 'describe', 'don', 'by', 'nine', 'himself', 'because', 'here', 'mill', 'and', 'else', 'upon', 'to', 'somewhere', 'afterwards', 'which', 'be', 'into', 'a', 'they', 'something', 'one', 'third', 'its', 'than', 'too', 'herein', 'themselves', 'whoever', 'wherever', 'were', 'among', 'mostly', 'system', 'latterly', 'whatever', 'never', 'seemed', 'hundred', 'thru', 'latter', 'from', 'whether', 'all', 'an', 'everything', 'below', 'done', 'indeed', 'your', 'detail', 'behind', 'then', 'thus', 'hereafter', 'ltd', 'under', 'nowhere', 'throughout', 'nothing', 'sometime', 'due', 'un', 'rather', 'etc', 'his', 'being', 'kg', 'the', 'nevertheless', 'although', 'along', 'out', 'for', 'go', 'quite', 'fifteen', 'hereby', 'towards', 'my', 'still', 'who', 'whereby', 'was', 'down', 'beyond', 'moreover', 'this', 'him', 'these', 'meanwhile', 'wherein', 'well', 'with', 'any', 'fifty', 'did', 'become', 'hers', 'someone', 'around', 'first', 'otherwise', 'anyhow', 'somehow', 'made', 'where', 'therein', 'keep', 'there', 'every', 'up', 'more', 'why', 'fill', 'through', 'elsewhere', 'six', 'had', 'thereby', 'anyone', 'sixty', 'or', 'various', 'that', 'everywhere', 'whither', 'again', 'it', 'seems', 'either', 'yet', 'call', 'thereupon', 'sincere', 'whereafter', 'please', 'hence', 'perhaps', 'is', 'less', 'became', 'have', 'beside', 'ie', 'eleven', 'couldnt', 'side', 'seeming', 'you', 'toward', 'much', 'bottom', 'serious', 'during', 'show', 'will', 'amoungst', 'at', 'anything', 'another', 'fire', 'alone', 'their', 'cannot', 'on', 'regarding', 'hereupon', 'say', 'anyway', 'whence', 'except', 'own', 'onto', 'amongst', 'above', 'am', 'becomes', 'five', 'itself', 'some', 'since', 'but', 'he', 'nobody', 'nor', 'us', 'whenever', 'unless', 'empty', 'everyone', 'when', 'next', 'few', 'full', 'thence', 'thereafter', 'together', 'bill', 'neither', 'i', 'hasnt', 'becoming', 'each', 'further', 'sometimes', 'almost', 'several', 'interest', 'give', 'cant', 'back', 'same', 'get', 'ourselves', 'herself', 'km', 'least', 'after', 'those', 'we', 'co', 'whole', 'does', 'about', 'really', 'once', 'across', 'ours', 'before', 'found', 'until', 'beforehand', 'so', 'amount', 'doesn', 'top', 'whose', 'also', 'against', 'thick', 'con', 'whereupon', 'move', 'doing', 'ever', 'may', 'noone', 'name', 'find', 'yourselves', 'should', 'such', 'none', 'computer', 'most', 'over', 'them', 'might', 'without', 'while', 'others', 'only', 'would', 'though', 'make', 'eight', 'eg', 'didn', 'very', 'do', 'cry', 'other', 'namely', 'take', 'both', 'thin', 'put', 'using', 'if', 'can', 'enough', 'many', 'three', 'seem', 'whereas', 'been', 'yours', 'former', 'always', 'how', 'not', 'four', 'as', 'see', 'part', 'what', 'ten', 'de', 'in', 'off', 'twelve', 'has', 'twenty', 'even', 'front', 'yourself', 'mine', 'forty', 'used', 'two', 'inc', 'must', 'formerly', 'she', 'therefore', 'whom', 'already', 'could', 'are', 're', 'via', 'of', 'myself', 'her', 'within'})\n",
      "['Yashesh', 'football', ',', 'fond', 'tennis', '.']\n",
      "['Yashesh', 'likes', 'play', 'football', ',', 'not', 'fond', 'tennis', '.']\n"
     ]
    }
   ],
   "source": [
    "#pip install gensim\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "text = \"Yashesh likes to play football, however he is not too fond of tennis.\"\n",
    "filtered_sentence = remove_stopwords(text)\n",
    "print(filtered_sentence)\n",
    "all_stopwords = gensim.parsing.preprocessing.STOPWORDS\n",
    "print(all_stopwords)\n",
    "'''The following script adds likes and play to the list of stop words in Gensim:'''\n",
    "\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "all_stopwords_gensim = STOPWORDS.union(set(['likes', 'play']))\n",
    "\n",
    "text = \"Yashesh likes to play football, however he is not too fond of tennis.\"\n",
    "text_tokens = word_tokenize(text)\n",
    "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords_gensim]\n",
    "print(tokens_without_sw)\n",
    "\n",
    "'''Output:\n",
    "['Yashesh', 'football', ',', 'fond', 'tennis', '.']\n",
    "The following script removes the word \"not\" from the set of stop words in Gensim:'''\n",
    "\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "all_stopwords_gensim = STOPWORDS\n",
    "sw_list = {\"not\"}\n",
    "all_stopwords_gensim = STOPWORDS.difference(sw_list)\n",
    "text = \"Yashesh likes to play football, however he is not too fond of tennis.\"\n",
    "text_tokens = word_tokenize(text)\n",
    "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords_gensim]\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yaZciYHBHMi"
   },
   "source": [
    "***iii) Using Spacy Adding and Removing Stop Words in Default Spacy Stop Words List ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14614,
     "status": "ok",
     "timestamp": 1681899507399,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "rFe-XtV2BLnK",
    "outputId": "a1198893-47e3-4bfd-ba3a-99d8abf9f1b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yashesh', 'likes', 'football', ',', 'fond', 'tennis', '.']\n",
      "['Yashesh', 'likes', 'football', ',', 'not', 'fond', 'tennis', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#add the word play to the NLTK stop word collection\n",
    "all_stopwords = sp.Defaults.stop_words\n",
    "all_stopwords.add(\"play\")\n",
    "\n",
    "text = \"Yashesh likes to play football, however he is not too fond of tennis.\"\n",
    "text_tokens = word_tokenize(text)\n",
    "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
    "\n",
    "print(tokens_without_sw)\n",
    "\n",
    "#remove 'not' from stop word collection\n",
    "all_stopwords.remove('not')\n",
    "\n",
    "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
    "\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Practical - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zp5cg0ABaUD"
   },
   "source": [
    "# ***Practical - 4.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zp5cg0ABaUD"
   },
   "source": [
    "***a.Text Tokenization a. Tokenization using Python’s split() function ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1683641416914,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "lZ8fkfbuBu1l",
    "outputId": "9808b57c-811a-4024-8d43-1e60bad7294e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This tool is an a beta stage\n",
      " Alexa developers can use Get Metrics API to seamlessly analyse metric\n",
      "\n",
      "It also supports custom skill model, prebuilt Flash Briefing model, and the Smart Home Skill API\n",
      " You can use this\n",
      "tool for creation of monitors, alarms, and dashboards that spotlight changes\n",
      " The release of these three tools will\n",
      " enable developers to create visual rich skills for Alexa devices with screens\n",
      " Amazon describes these tools as the\n",
      "collection of tech and tools for creating visuallyrich and interactive voice experiences\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "text = \"\"\" This tool is an a beta stage. Alexa developers can use Get Metrics API to seamlessly analyse metric.\n",
    "It also supports custom skill model, prebuilt Flash Briefing model, and the Smart Home Skill API. You can use this\n",
    "tool for creation of monitors, alarms, and dashboards that spotlight changes. The release of these three tools will\n",
    " enable developers to create visual rich skills for Alexa devices with screens. Amazon describes these tools as the\n",
    "collection of tech and tools for creating visuallyrich and interactive voice experiences.  \"\"\"\n",
    "data = text.split('.')\n",
    "for i in data:\n",
    "  print (i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqeiAoPCB9N2"
   },
   "source": [
    "***b. Tokenization using Regular Expressions (RegEx) ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 471,
     "status": "ok",
     "timestamp": 1681899793470,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "ZlKwHYHBBvAG",
    "outputId": "7e7fb332-2adb-4e6e-b0d4-2ee212e1c9d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'to', 'study', 'Natural', 'Language', 'Processing', 'in', 'Python']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# import RegexpTokenizer() method from nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# Create a reference variable for Class RegexpTokenizer\n",
    "tk = RegexpTokenizer('\\s+', gaps = True)\n",
    "# Create a string input\n",
    "str = \"I love to study Natural Language Processing in Python\"\n",
    "# Use tokenize method\n",
    "tokens = tk.tokenize(str)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6v2Ax47CWw9"
   },
   "source": [
    "***c. Tokenization using NLTK ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 593,
     "status": "ok",
     "timestamp": 1681899817151,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "u7ppBFVECY-D",
    "outputId": "c853bb42-e216-4c76-9a42-d7152572c676"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'to', 'study', 'Natural', 'Language', 'Processing', 'in', 'Python']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Create a string input\n",
    "str = \"I love to study Natural Language Processing in Python\"\n",
    "# Use tokenize method\n",
    "print(word_tokenize(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Q6IVMjjCosP"
   },
   "source": [
    "*** d. Tokenization using the spaCy library ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 597,
     "status": "ok",
     "timestamp": 1681899952024,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "zpHHucvnCt1f",
    "outputId": "b001272b-43df-498d-f183-dc307aaefad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'to', 'study', 'Natural', 'Language', 'Processing', 'in', 'Python']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create a string input\n",
    "str = \"I love to study Natural Language Processing in Python\"\n",
    "\n",
    "# Create an instance of document;\n",
    "# doc object is a container for a sequence of Token objects.\n",
    "doc = nlp(str)\n",
    "\n",
    "# Read the words; Print the words\n",
    "words = [word.text for word in doc]\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccfPUGXVDHai"
   },
   "source": [
    "*** e. Tokenization using Keras ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1681900019291,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "FDJRsZbcDOuZ",
    "outputId": "29e75d18-0237-41f7-9a20-355b14611731"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'to', 'study', 'natural', 'language', 'processing', 'in', 'python']\n"
     ]
    }
   ],
   "source": [
    "#pip install keras #pip install tensorflow\n",
    "import keras\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# Create a string input\n",
    "str = \"I love to study Natural Language Processing in Python\"\n",
    "\n",
    "# tokenizing the text\n",
    "tokens = text_to_word_sequence(str)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zqho9mmyDat7"
   },
   "source": [
    "*** f. Tokenization using Gensim ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 473,
     "status": "ok",
     "timestamp": 1681900076207,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "IuhDRdMGDfrY",
    "outputId": "71e46d00-fbc7-4669-b607-1eb740564019"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'love',\n",
       " 'to',\n",
       " 'study',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'in',\n",
       " 'Python']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pip install gensim\n",
    "\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "# Create a string input\n",
    "str = \"I love to study Natural Language Processing in Python\"\n",
    "\n",
    "# tokenizing the text\n",
    "list(tokenize(str))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOR6RROp8fRsXhT6lfvLcjG",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
