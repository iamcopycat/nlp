{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJPOR9loC3fW"
   },
   "source": [
    "# ***Practical - 5***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJPOR9loC3fW"
   },
   "source": [
    "***a) word tokenization in Hindi ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1640,
     "status": "ok",
     "timestamp": 1687855791850,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "4HCnzdd2Cz9R",
    "outputId": "ae35c107-6dd5-43e9-d60b-714576ac7e90"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['सुबह', 'उठकर', 'समय', 'पर', 'खाना', 'खाना', 'स्वस्थ', 'जीवन', 'जीने', 'के', 'लिए', 'बहुत', 'जरूरी', 'है।']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Example Hindi sentence\n",
    "sentence = 'सुबह उठकर समय पर खाना खाना स्वस्थ जीवन जीने के लिए बहुत जरूरी है।'\n",
    "# Perform word tokenization on the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "# Print the output tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k40qGD415GRm"
   },
   "source": [
    "***c) Identify the Indian language of a text ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1012,
     "status": "ok",
     "timestamp": 1683739255994,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "Idh7xtEnrF2S",
    "outputId": "c8d784f5-d045-44b9-987c-97166e10a5d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: hi\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "# example text\n",
    "text = \"हिन्दी मेरी मातृभाषा है।\"\n",
    "\n",
    "# detect language\n",
    "lang = detect(text)\n",
    "\n",
    "# print detected language\n",
    "print(\"Detected language:\", lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e77z-OOcG-gd"
   },
   "source": [
    "# ***Practical - 10***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e77z-OOcG-gd"
   },
   "source": [
    "***a. Speech Tagging:*** \n",
    "\n",
    "***i. Speech tagging using spacy ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83eV97oTG3dX"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "sen = sp(u\"I like to play football. I hated it in my childhood though\")\n",
    "print(sen.text)\n",
    "print(sen[7].pos_)\n",
    "print(sen[7].tag_)\n",
    "print(spacy.explain(sen[7].tag_))\n",
    "for word in sen:\n",
    "  print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')\n",
    "sen = sp(u'Can you google it?')\n",
    "word = sen[2]\n",
    "\n",
    "print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')\n",
    "sen = sp(u'Can you search it on google?')\n",
    "word = sen[5]\n",
    "print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')\n",
    "\n",
    "#Finding the Number of POS Tags\n",
    "sen = sp(u\"I like to play football. I hated it in my childhood though\")\n",
    "num_pos = sen.count_by(spacy.attrs.POS)\n",
    "num_pos\n",
    "\n",
    "for k,v in sorted(num_pos.items()):\n",
    "  print(f'{k}. {sen.vocab[k].text:{8}}: {v}')\n",
    "#Visualizing Parts of Speech Tags\n",
    "from spacy import displacy\n",
    "\n",
    "sen = sp(u\"I like to play football. I hated it in my childhood though\")\n",
    "displacy.serve(sen, style='dep', options={'distance': 120})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8EdytKEJpiq"
   },
   "source": [
    "**ii. Speech tagging using NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1683824311960,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "jRJvThQ9Ili7",
    "outputId": "9f74ba17-bc5c-4fc7-b683-dd9c3809d0cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "#create our training and testing data:\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "#train the Punkt tokenizer like:\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "# tokenize:\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "  try:\n",
    "    for i in tokenized[:2]:\n",
    "      words = nltk.word_tokenize(i)\n",
    "      tagged = nltk.pos_tag(words)\n",
    "      print(tagged)\n",
    "\n",
    "  except Exception as e:\n",
    "    print(str(e))\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 644,
     "status": "ok",
     "timestamp": 1683824310164,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "0bKQwfwPv9Gw",
    "outputId": "b80cc187-0dfb-44b9-dd71-bac5491472b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 402,
     "status": "ok",
     "timestamp": 1683824245315,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "7Y4wdTn_vr7H",
    "outputId": "49dd0178-1c97-4505-9c5f-fd39cd84a8bd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/state_union.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('state_union')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1683824286473,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "YtrEK_aBv3m_",
    "outputId": "1122d681-1d03-400f-bd7c-a82d98991e44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0K2bwOhFJxaM"
   },
   "source": [
    "***b. Statistical parsing:*** \n",
    "\n",
    "***i. Usage of Give and Gave in the Penn Treebank sample ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "executionInfo": {
     "elapsed": 576,
     "status": "error",
     "timestamp": 1687509777377,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "V6kYAtHbJ0Gv",
    "outputId": "ed783d11-7653-498d-93fd-daa5520cf779"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-4fb4b093e676>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    output = \"%s %s: %s / %s: %s\" %\\\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.parse.viterbi\n",
    "import nltk.parse.pchart\n",
    "\n",
    "def give(t):\n",
    "  return t.label() == 'VP' and len(t) > 2 and t[1].label() == 'NP'\\\n",
    "  and (t[2].label() == 'PP-DTV' or t[2].label() == 'NP')\\\n",
    "  and ('give' in t[0].leaves() or 'gave' in t[0].leaves())\n",
    "\n",
    "def sent(t):\n",
    "  return ' '.join(token for token in t.leaves() if token[0] not in '*-0')\n",
    "\n",
    "def print_node(t, width):\n",
    "  output = \"%s %s: %s / %s: %s\" %\\\n",
    "  (sent(t[0]), t[1].label(), sent(t[1]), t[2].label(), sent(t[2]))\n",
    "  if len(output) > width:\n",
    "   output = output[:width] + \"...\"\n",
    "  print (output)\n",
    "\n",
    "for tree in nltk.corpus.treebank.parsed_sents():\n",
    "  for t in tree.subtrees(give):\n",
    "    print_node(t, 72)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNywn7xFKu46"
   },
   "source": [
    "***ii. probabilistic parser ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 535,
     "status": "ok",
     "timestamp": 1687507302915,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "_FB6P90MKx7J",
    "outputId": "0199f0b1-5dba-44ee-d8c7-9d597c4cc637"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 11 productions (start state = NP)\n",
      "    NP -> NNS [0.5]\n",
      "    NP -> JJ NNS [0.3]\n",
      "    NP -> NP CC NP [0.2]\n",
      "    NNS -> 'men' [0.1]\n",
      "    NNS -> 'women' [0.2]\n",
      "    NNS -> 'children' [0.3]\n",
      "    NNS -> NNS CC NNS [0.4]\n",
      "    JJ -> 'old' [0.4]\n",
      "    JJ -> 'young' [0.6]\n",
      "    CC -> 'and' [0.9]\n",
      "    CC -> 'or' [0.1]\n",
      "Output: \n",
      "(NP (JJ old) (NNS (NNS men) (CC and) (NNS women))) (p=0.000864)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import PCFG\n",
    "\n",
    "grammar = PCFG.fromstring('''\n",
    "NP -> NNS [0.5] | JJ NNS [0.3] | NP CC NP [0.2]\n",
    "NNS -> \"men\" [0.1] | \"women\" [0.2] | \"children\" [0.3] | NNS CC NNS [0.4]\n",
    "JJ -> \"old\" [0.4] | \"young\" [0.6]\n",
    "CC -> \"and\" [0.9] | \"or\" [0.1]\n",
    "''')\n",
    "\n",
    "print(grammar)\n",
    "viterbi_parser = nltk.ViterbiParser(grammar)\n",
    "token = \"old men and women\".split()\n",
    "obj = viterbi_parser.parse(token)\n",
    "\n",
    "print(\"Output: \")\n",
    "for x in obj:\n",
    "  print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1503,
     "status": "ok",
     "timestamp": 1687507148987,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "wClwrdCQMP2N",
    "outputId": "557d314d-37b4-4679-c2c3-98c30cfac982"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading PCFG: Package 'PCFG' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('PCFG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dykORXR1cSCg"
   },
   "source": [
    "# ***Practical 11***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dykORXR1cSCg"
   },
   "source": [
    "***a) Multiword Expressions in NLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1314,
     "status": "ok",
     "timestamp": 1683735202178,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "1bIewzGjbyUM",
    "outputId": "79a72f6b-c097-4b4b-fc8d-02224e48354c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'cake', 'cost', 'Rs.1500\\\\kg', 'in', 'Mumbai', '.']\n",
      "['Please', 'buy', 'me', 'one', 'of', 'them', '.']\n",
      "['Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "# Multiword Expressions in NLP\n",
    "\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "s = '''Good cake cost Rs.1500\\kg in Mumbai.  Please buy me one of them.\\n\\nThanks.'''\n",
    "mwe = MWETokenizer([('New', 'York'), ('Hong', 'Kong')], separator='_')\n",
    "for sent in sent_tokenize(s):\n",
    "  print(mwe.tokenize(word_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1374,
     "status": "ok",
     "timestamp": 1683735197735,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "1uM-P057cBXb",
    "outputId": "b2e56f57-5f0b-4875-b6dd-58e1cde7bd7a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8A4jEOrecV_p"
   },
   "source": [
    "***b) Normalized Web Distance and Word Similarity ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1687511034429,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "6OdtYY0LW06p",
    "outputId": "350146d3-5089-4efe-9138-19241cbae48a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reliance', 'reliance', 'reliance', 'reliance', 'reliance', 'reliance', 'mumbai', 'mumbai', 'mumbai', 'mumbai', 'km trading', 'km trading', 'km trading', 'km trading', 'km trading']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import textdistance\n",
    "# pip install textdistance # we will need scikit-learn>=0.21\n",
    "import sklearn #pip install sklearn\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "texts = [   'Reliance supermarket', 'Reliance hypermarket', 'Reliance', 'Reliance', 'Reliance downtown', 'Relianc market',    'Mumbai', 'Mumbai Hyper', 'Mumbai dxb', 'mumbai airport',     'k.m trading', 'KM Trading', 'KM trade', 'K.M.  Trading', 'KM.Trading' ]\n",
    "\n",
    "def normalize(text):\n",
    "  \"\"\" Keep only lower-cased text and numbers\"\"\"\n",
    "  return re.sub('[^a-z0-9]+', ' ', text.lower())\n",
    "\n",
    "def group_texts(texts, threshold=0.4):\n",
    "  \"\"\" Replace each text with the representative of its cluster\"\"\"\n",
    "  normalized_texts = np.array([normalize(text) for text in texts])\n",
    "  distances = 1 - np.array([\n",
    "      [textdistance.jaro_winkler(one, another) for one in normalized_texts]\n",
    "      for another in normalized_texts   ])\n",
    "  clustering = AgglomerativeClustering(\n",
    "      distance_threshold=threshold, # this parameter needs to be tuned carefully\n",
    "      affinity=\"precomputed\", linkage=\"complete\", n_clusters=None   ).fit(distances)\n",
    "  centers = dict()\n",
    "\n",
    "  for cluster_id in set(clustering.labels_):\n",
    "    index = clustering.labels_ == cluster_id\n",
    "    centrality = distances[:, index][index].sum(axis=1)\n",
    "    centers[cluster_id] = normalized_texts[index][centrality.argmin()]\n",
    "  return [centers[i] for i in clustering.labels_]\n",
    "\n",
    "print(group_texts(texts))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5948,
     "status": "ok",
     "timestamp": 1687511008626,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "nX3UtZkfd886",
    "outputId": "d336e705-1fa9-4f1a-9264-4f20b09bf765"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting textdistance\n",
      "  Downloading textdistance-4.5.0-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: textdistance\n",
      "Successfully installed textdistance-4.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iC_yAX6BfIwe"
   },
   "source": [
    "***c) Word Sense Disambiguation ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1969,
     "status": "ok",
     "timestamp": 1683736139314,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "x8N0XIqpfLf7",
    "outputId": "40f375cd-8e1d-420a-9cfa-92888d005faf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Synset.name of Synset('bank.n.01')>: <bound method Synset.definition of Synset('bank.n.01')>\n",
      "<bound method Synset.name of Synset('set.n.01')>: <bound method Synset.definition of Synset('set.n.01')>\n",
      "<bound method Synset.name of Synset('put.v.01')>: <bound method Synset.definition of Synset('put.v.01')>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_first_sense(word, pos=None):\n",
    "  if pos:\n",
    "    synsets = wn.synsets(word,pos)\n",
    "  else:\n",
    "    synsets = wn.synsets(word)\n",
    "  return synsets[0]\n",
    "\n",
    "best_synset = get_first_sense('bank')\n",
    "print ('%s: %s' % (best_synset.name, best_synset.definition))\n",
    "best_synset = get_first_sense('set','n')\n",
    "print ('%s: %s' % (best_synset.name, best_synset.definition))\n",
    "best_synset = get_first_sense('set','v')\n",
    "print ('%s: %s' % (best_synset.name, best_synset.definition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2U6Z6eithm2-"
   },
   "outputs": [],
   "source": [
    " Generate similar sentences from a given Hindi text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 400,
     "status": "ok",
     "timestamp": 1683736134541,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "VUCmLHjZfmOc",
    "outputId": "cc69f25c-9fb0-4540-e3f4-229db132cfe5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPmj1Wgd0gSh1D9LBLrPHe6",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
