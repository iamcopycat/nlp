{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0yUB2uVjYoS"
   },
   "source": [
    "# ***Practical  - 7***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0yUB2uVjYoS"
   },
   "source": [
    "***Finite state automata***\n",
    " \n",
    "***a) Define grammar using nltk. Analyze a sentence using the same. ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Z4JaQXtLj5j"
   },
   "source": [
    "***a) Define grammar using nltk. Analyze a sentence using the same. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66CvhitjhQfT"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> VP\n",
    "  VP -> VP NP\n",
    "  NP -> Det  NP\n",
    "  Det -> 'that'\n",
    "  NP -> singular Noun\n",
    "  NP -> 'flight'\n",
    "  VP -> 'Book'\n",
    "  \"\"\")\n",
    "sentence = \"Book that flight\"\n",
    "\n",
    "for index in range(len(sentence)):\n",
    "  all_tokens = tokenize.word_tokenize(sentence)\n",
    "  print(all_tokens)\n",
    "\n",
    "parser = nltk.ChartParser(grammar1)\n",
    "for tree in parser.parse(all_tokens):\n",
    "  print(tree)\n",
    "  tree.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3p8ZA6HK3Uh"
   },
   "source": [
    "***b) Accept the input string with Regular expression of Finite Automaton: 101+. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1683819618729,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "4UHoCBJGESAH",
    "outputId": "9005f2ed-a4dd-4e98-a898-084d36e3a1bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected\n",
      "Rejected\n",
      "Accepted\n",
      "Accepted\n",
      "Rejected\n",
      "Rejected\n",
      "Rejected\n",
      "Rejected\n",
      "Accepted\n"
     ]
    }
   ],
   "source": [
    "def FA(s):\n",
    "   #if the length is less than 3 then it can't be accepted, Therefore end the process.\n",
    "   if len(s)<3:\n",
    "    return \"Rejected\"\n",
    "   if s[0]=='1':\n",
    "    if s[1]=='0':\n",
    "      if s[2]=='1':\n",
    "        for i in range(3,len(s)):\n",
    "          if s[i]!='1':\n",
    "            return \"Rejected\"\n",
    "        return \"Accepted\" # if all 4 nested if true\n",
    "      return \"Rejected\" # else of 3rd if\n",
    "    return \"Rejected\" # else of 2nd if\n",
    "   return \"Rejected\" # else of 1st if\n",
    "inputs=['1','10101','101','10111','01010','100','','10111101','1011111']\n",
    "for i in inputs:\n",
    "  print(FA(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 548,
     "status": "ok",
     "timestamp": 1683819720450,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "BriOHFSujD21",
    "outputId": "d10e85d0-3f66-4060-e587-15983d30554a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEDoE_sxLNIR"
   },
   "source": [
    "***c) Accept the input string with Regular expression of FA:*** (a+b)*bba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 732,
     "status": "ok",
     "timestamp": 1683820145471,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "zBKWQm0TewAB",
    "outputId": "79fd2a25-79df-41f9-eae8-e6cde9deb7cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted\n",
      "Accepted\n",
      "Accepted\n",
      "Rejected\n",
      "Rejected\n",
      "Rejected\n",
      "Rejected\n"
     ]
    }
   ],
   "source": [
    "def FA(s):\n",
    "  size=0 #scan complete string and make sure that it contains only 'a' & 'b'\n",
    "  for i in s:\n",
    "    if i=='a' or i=='b':\n",
    "      size+=1\n",
    "    else:\n",
    "      return \"Rejected\"\n",
    "  if size>=3:#check the last 3 elements\n",
    "     if s[size-3]=='b':\n",
    "      if s[size-2]=='b':\n",
    "       if s[size-1]=='a':\n",
    "         return \"Accepted\"\n",
    "       return \"Rejected\"\n",
    "      return \"Rejected\"\n",
    "     return \"Rejected\"\n",
    "  return \"Rejected\" # else of 1st if\n",
    "\n",
    "inputs=['bba', 'ababbba', 'abba','abb', 'baba','bbb','']\n",
    "for i in inputs:\n",
    "  print(FA(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koTTO58MLGU-"
   },
   "source": [
    "***d) Implementation of Deductive Chart Parsing using context free grammar and a given sentence. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BAppCWXMgYzY"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  PP -> P NP\n",
    "  NP -> Det N | Det N PP | 'I'\n",
    "  VP -> V NP | VP PP\n",
    "  Det -> 'a' | 'my'\n",
    "  N -> 'bird' | 'balcony'\n",
    "  V -> 'saw'\n",
    "  P -> 'in'\n",
    "  \"\"\")\n",
    "sentence = \"I saw a bird in my balcony\"\n",
    "\n",
    "for index in range(len(sentence)):\n",
    "  all_tokens = tokenize.word_tokenize(sentence)\n",
    "  print(all_tokens)\n",
    "\n",
    "# all_tokens = ['I', 'saw', 'a', 'bird', 'in', 'my', 'balcony']\n",
    "parser = nltk.ChartParser(grammar1)\n",
    "for tree in parser.parse(all_tokens):\n",
    "  print(tree)\n",
    "  tree.draw()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical - 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-YL0FtyFcu9"
   },
   "source": [
    "***8. Study PorterStemmer, LancasterStemmer, RegexpStemmer, SnowballStemmer Study WordNetLemmatizer ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1683821283795,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "Sxh32wmYFikT",
    "outputId": "dc8b0ca6-9fc8-400e-ef1f-7c7ae0bb040d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write\n"
     ]
    }
   ],
   "source": [
    "# PorterStemmer\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "word_stemmer = PorterStemmer()\n",
    "print(word_stemmer.stem('writing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 438,
     "status": "ok",
     "timestamp": 1683821289902,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "21mfcxSwF2_l",
    "outputId": "3aedb9f2-bbfc-4af6-c3ef-459bd142e42b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writ\n"
     ]
    }
   ],
   "source": [
    "#LancasterStemmer\n",
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "Lanc_stemmer = LancasterStemmer()\n",
    "print(Lanc_stemmer.stem('writing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1683821292783,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "W6dZe3V8GBEP",
    "outputId": "0d1f09a4-1e10-460f-b662-0e86cdda2ccc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writ\n"
     ]
    }
   ],
   "source": [
    "#RegexpStemmer\n",
    "import nltk\n",
    "from nltk.stem import RegexpStemmer\n",
    "Reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "print(Reg_stemmer.stem('writing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1683821294861,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "tgjrwhC6GNzO",
    "outputId": "6003d9b8-1160-49d8-8810-37de7c89ae34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write\n"
     ]
    }
   ],
   "source": [
    "#SnowballStemmer\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "english_stemmer = SnowballStemmer('english')\n",
    "print(english_stemmer.stem ('writing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 498,
     "status": "ok",
     "timestamp": 1683821307900,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "K_VLjbjLGody",
    "outputId": "76411c77-ab15-4654-e8c0-cd5cd67bb5e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1689,
     "status": "ok",
     "timestamp": 1683821313958,
     "user": {
      "displayName": "Sneha Kadam",
      "userId": "00740706004237501599"
     },
     "user_tz": -330
    },
    "id": "MfnDot9UGT7B",
    "outputId": "806d4083-fda1-4ade-bc59-a922145b8990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word :\tlemma\n",
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "#WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(\"word :\\tlemma\")\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNyDH3PZTYMiQudoRw5S55m",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
